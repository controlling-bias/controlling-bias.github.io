---
layout: default
---

*	[<b> Diverse Representation via Computational Participatory Elections – Lessons from a Case Study</b>](https://arxiv.org/abs/2205.15394)\\
	Florian Evéquoz, Johan Rochel, Vijay Keswani, L. Elisa Celis\\
	EAAMO 2022 

*	<b>Fair Ranking with Noisy Protected Attributes</b>\\
	Anay Mehrotra and Nisheeth K. Vishnoi\\
	NeurIPS 2022 

*	<b>Revisiting Group Fairness Metrics: The Effect of Networks</b>\\
	Anay Mehrotra, Jeffrey Sachs, and L. Elisa Celis\\
	CSCW 2022 
	
*	[<b>Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints</b>](https://arxiv.org/abs/2202.01661)\\
	Anay Mehrotra, Bary S. R. Pradelski, and Nisheeth K. Vishnoi\\
	ACM FAccT 2022 [[short-video]](https://www.youtube.com/watch?v=1FC-am5XcJ8) [[long-video]](https://www.youtube.com/watch?v=lc3ekRrp4JA) 

*	[<b>Fairness for AUC via Feature Augmentation</b>](https://arxiv.org/abs/2111.12823)\\
	Hortense Fong, Vineet Kumar, Anay Mehrotra, Nisheeth K. Vishnoi\\
	ACM FAccT 2022 [[short-video]](https://www.youtube.com/watch?v=gb0hz3_hKGM) [[long-video]](https://www.youtube.com/watch?v=Q3DeQIpAIYc) 

*	[<b>Fair Classification with Adversarial Perturbations</b>](https://arxiv.org/abs/2106.05964)\\
	L. Elisa Celis, Anay Mehrotra, Nisheeth K. Vishnoi\\
	NeurIPS 2021 [[code]](https://github.com/AnayMehrotra/Fair-classification-with-adversarial-perturbations) [[video]](https://www.dropbox.com/s/ibi5bzqgdk4m5aw/neurips-final-talk.mp4?dl=0) 
	
*	[<b>Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees</b>](https://arxiv.org/abs/2006.04778)\\
	L. Elisa Celis, Lingxiao Huang, Vijay Keswani, Nisheeth K. Vishnoi\\
	ICML 2021 [[code]](https://github.com/vijaykeswani/Noisy-Fair-Classification)
	
*	[<b>Auditing for Diversity using Representative Examples</b>](https://arxiv.org/abs/2107.07393)\\
	Vijay Keswani, L. Elisa Celis\\
	ACM SIGKDD 2021 	
	
*	[<b>Towards Unbiased and Accurate Deferral to Multiple Experts</b>](http://arxiv.org/abs/2102.13004)\\
	Vijay Keswani, Matthew Lease, Krishnaram Kenthapadi\\
	AIES 2021 
	
*	[<b>Dialect Diversity in Text Summarization on Twitter</b>](https://arxiv.org/abs/2007.07860)\\
	Vijay Keswani, L. Elisa Celis\\
	The Web Conference (formerly WWW) 2021 

*	[<b>Mitigating Bias in Set Selection with Noisy Protected Attributes</b>](https://arxiv.org/abs/2011.04219)\\
	Anay Mehrotra, L. Elisa Celis\\
	ACM FAccT 2021 [[code]](https://github.com/AnayMehrotra/Noisy-Fair-Subset-Selection) [[video]](https://www.youtube.com/watch?v=ZzvSPhL5ZyQ)

*	[<b>The Effect of the Rooney Rule on Implicit Bias in the Long Term</b>](https://arxiv.org/abs/2010.10992)\\
	L. Elisa Celis, Chris Hays, Anay Mehrotra, Nisheeth K. Vishnoi\\
	ACM FAccT 2021 [[demo]](https://downstreamrooney.herokuapp.com/experiment/iteration/) [[demo-code]](https://github.com/johnchrishays/downstream-rooney-mturk) [[video]](https://www.youtube.com/watch?v=hLTIEzb5A-A) 

*	[<b>Implicit Diversity in Image Summarization</b>](https://arxiv.org/abs/1901.10265)\\
	L. Elisa Celis, Vijay Keswani\\
	CSCW 2020

*	[<b>Data Preprocessing to Mitigate Bias: A Maximum Entropy-based Approach</b>](https://arxiv.org/abs/1906.02164)\\
	L. Elisa Celis, Vijay Keswani, Nisheeth K. Vishnoi\\
	ICML 2020 [[blogpost]](https://www.computationsociety.org/2020/07/27/max-entropy/) [[code]](https://github.com/vijaykeswani/Fair-Max-Entropy-Distributions)

*	[<b>Towards Just, Fair and Interpretable Methods for Judicial Subset Selection</b>](https://dl.acm.org/doi/10.1145/3375627.3375848)\\
	Lingxiao Huang, Julia Wei, L. Elisa Celis\\
	AIES 2020

*	[<b>Interventions for Ranking in the Presence of Implicit Bias</b>](https://arxiv.org/abs/2001.08767)\\
	L. Elisa Celis, Anay Mehrotra, Nisheeth K. Vishnoi\\
	ACM FAccT (formerly FAT*) 2020 [[video]](https://www.youtube.com/watch?v=Mkc2pTMo2Lg&list=PLXA0IWa3BpHkaM6uJauBdtkR3Hp4rePfp) [[code]](https://github.com/AnayMehrotra/Ranking-with-Implicit-Bias)

*	[<b>Assessing Social and Intersectional Biases in Contextualized Word Representations</b>](https://arxiv.org/abs/1911.01485)\\
	Yi Chern Tan, L. Elisa Celis\\
	NeurIPS 2019

*	[<b>Coresets for Clustering with Fairness Constraints</b>](https://arxiv.org/abs/1906.08484)\\
	Lingxiao Huang, Shaofeng Jiang, Nisheeth K. Vishnoi\\
	NeurIPS 2019

*	[<b>Towards Controlling Discrimination in Online Ad Auctions</b>](https://arxiv.org/abs/1901.10450)\\
	L. Elisa Celis, Anay Mehrotra, Nisheeth K. Vishnoi\\
	ICML 2019 [[demo]](https://fair-online-advertising.herokuapp.com/) [[methodology]](http://cs.yale.edu/bias/blog/jekyll/update/2019/02/08/fair-advertising.html) [[code]](https://github.com/AnayMehrotra/Fair-Online-Advertising) [[video: 26m-31m]](https://slideslive.com/38917642/privacy-and-fairness)\\
	(Also <b> best student paper </b> in 3rd Workshop on Mechanism Design for Social Good 2019) 

*	[<b>Stable and Fair Classification</b>](https://arxiv.org/abs/1902.07823)\\
	Lingxiao Huang, Nisheeth K. Vishnoi\\
	ICML 2019 [[code]](https://github.com/huanglx12/Stable-Fair-Classification)

*	<b>A Dashboard for Controlling Polarization in Personalization</b>\\
	L. Elisa Celis, Sayash Kapoor, Farnood Salehi, Vijay Keswani, Nisheeth K. Vishnoi\\
	<b> Invited paper </b> in AI Communications 2019 [[demo]](https://fair-personalized-news.herokuapp.com/fast/diversity.php) [[methodology]](http://cs.yale.edu/bias/blog/jekyll/update/2018/01/20/balanced-news-search.html) \\
	(Previously in [IJCAI-ECAI](https://www.ijcai.org/Proceedings/2018/854) 2018) 

*	[<b>Controlling Polarization in Personalization</b>](http://arxiv.org/abs/1802.08674)\\
	L. Elisa Celis, Sayash Kapoor, Farnood Salehi, Nisheeth K. Vishnoi\\
	<b> Best technical paper </b> - ACM FAccT (formerly FAT*) 2019 [[video: 19m-29m]](https://www.youtube.com/watch?v=m8MjSmtrMSg)

*	[<b>Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees</b>](https://arxiv.org/abs/1806.06055)\\
	L. Elisa Celis, Lingxiao Huang, Vijay Keswani, Nisheeth K. Vishnoi\\
	ACM FAccT (formerly FAT*) 2019 [[blogpost]](http://cs.yale.edu/bias/blog/jekyll/update/2018/11/06/fair-classification.html) [[code]](https://github.com/vijaykeswani/FairClassification) [[video: 11m-21m]](https://www.youtube.com/watch?v=_oue1AgiQ_g)

*	[<b>Fair and Diverse DPP-based Data Summarization</b>](https://arxiv.org/abs/1802.04023)\\
	L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria, Nisheeth K. Vishnoi\\
	ICML 2018 [[demo]](https://fair-image-search.herokuapp.com/imageDiversity.php) [[code]](https://github.com/DamianStraszak/FairDiverseDPPSampling) [[video]](https://vimeo.com/295743995)

*	[<b>Multiwinner Voting with Fairness Constraints</b>](http://arxiv.org/abs/1710.10057)\\
	L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi\\
	IJCAI-ECAI 2018 [[demo]](https://fair-voting-demo.herokuapp.com/) [[blogpost]](https://nisheethvishnoi.wordpress.com/2018/09/16/fair-elections/) [[code]](https://github.com/huanglx12/Balanced-Committee-Election)
	
	
*	[<b>Ranking with Fairness Constraints</b>](https://arxiv.org/abs/1704.06840)\\
	L. Elisa Celis, Damian Straszak, Nisheeth K. Vishnoi\\
	ICALP 2018 [[demo]](http://balanced-ranking.herokuapp.com/) [[methodology]](http://cs.yale.edu/bias/blog/jekyll/update/2018/11/03/balanced-ranking.html)

*	[<b>On the Complexity of Constrained Determinantal Point Processes</b>](https://drops.dagstuhl.de/opus/volltexte/2017/7585/)\\
	L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, Nisheeth K. Vishnoi\\
	APPROX-RANDOM 2018 


*	[<b>Fair Personalization</b>](http://arxiv.org/abs/1707.02260)\\
	L. Elisa Celis, Nisheeth K. Vishnoi\\
	FAT ML 2017

*	[<b>How to be Fair and Diverse?</b>](https://arxiv.org/abs/1610.07183)\\
	L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Nisheeth K. Vishnoi\\
	FAT ML 2016 [[video]](https://www.fatml.org/schedule/2016/presentation/how-be-fair-and-diverse)

